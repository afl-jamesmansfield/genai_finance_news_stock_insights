{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOOORjFnMLUob0A9dKp/v/5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afl-jamesmansfield/genai_finance_news_stock_insights/blob/main/genai_finance_usecase_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI use case in finance\n",
        "Use case analytizing news and financial stock prices to provide summary based on company stock.\n",
        "Use case demonstrates generative ai technology skill using genai google and platform analytics to develop gen ai models at scale and with multiple structured and unstructured data sets.\n"
      ],
      "metadata": {
        "id": "Wrz1kNdLr9qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "install google gen ai"
      ],
      "metadata": {
        "id": "gx2Ieg92sWdV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wu17Wtnqr1zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092fc6b7-e389-4630-95a1-d907bce70909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/241.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.7/241.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q -U google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gemini api key"
      ],
      "metadata": {
        "id": "dRf1HDOAsg3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env GEMINI_API_KEY = [ENTER GOOGLE GEMINI API KEY]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w125FHBtsnCm",
        "outputId": "9c1e1353-91fd-40bc-bf08-02a1b4e36166"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GEMINI_API_KEY=AIzaSyAW2sUVYBdlyLIcWJTmwtMXY91hFwtQKjg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data input; using alpha advantage illustrative news source"
      ],
      "metadata": {
        "id": "5wtRUUrZtOKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env ALPHAVANTAGE_API_KEY = [ENTER ALPHAVANTAGE API KEY]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ1qyr7gtUM2",
        "outputId": "64c2700c-65b3-4d21-8902-5c306b048353"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: ALPHAVANTAGE_API_KEY=0HLXGEWG8SGCKA3V\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "install google gemini llm model"
      ],
      "metadata": {
        "id": "fQcZwEnltpFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain how AI works in a few words\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.2,\n",
        "        max_output_tokens=300,\n",
        "        top_p=0.9,  # Added top_p\n",
        "        top_k=40,   # Added top_k\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking\n",
        "    ),\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZz3IijFtY0A",
        "outputId": "f7e5fdec-1334-4e58-a3e7-6fe02a64adcc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI works by **learning patterns from data to make predictions or decisions.**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Company-news Q&A chatbot (robust v2, fixed)\n",
        "\n",
        "- News: Alpha Vantage NEWS_SENTIMENT\n",
        "- Ticker search: Alpha Vantage SYMBOL_SEARCH\n",
        "- LLM: Gemini (google-genai)\n",
        "\n",
        "Setup:\n",
        "  pip install google-genai requests\n",
        "  export GOOGLE_API_KEY=...\n",
        "  export ALPHAVANTAGE_API_KEY=...\n",
        "  # optional debugging\n",
        "  export DEBUG_NEWS_BOT=1\n",
        "\n",
        "Run:\n",
        "  python news_qa_bot_v2.py\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yb55F0smuHPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Company-news Q&A chatbot (robust v2, fixed)\n",
        "\n",
        "- News: Alpha Vantage NEWS_SENTIMENT\n",
        "- Ticker search: Alpha Vantage SYMBOL_SEARCH\n",
        "- LLM: Gemini (google-genai)\n",
        "\n",
        "Setup:\n",
        "  pip install google-genai requests\n",
        "  export GOOGLE_API_KEY=...\n",
        "  export ALPHAVANTAGE_API_KEY=...\n",
        "  # optional debugging\n",
        "  export DEBUG_NEWS_BOT=1\n",
        "\n",
        "Run:\n",
        "  python news_qa_bot_v2.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# --- Gemini client ---\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "GEMINI_MODEL = \"gemini-2.5-flash\"\n",
        "AV_BASE = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "client = genai.Client()  # reads GOOGLE_API_KEY from env\n",
        "DEBUG = os.environ.get(\"DEBUG_NEWS_BOT\") == \"1\"\n",
        "\n",
        "def dprint(*args):\n",
        "    if DEBUG:\n",
        "        print(\"[debug]\", *args)\n",
        "\n",
        "# --- Utilities ---------------------------------------------------------------\n",
        "\n",
        "def av_time(dt: datetime) -> str:\n",
        "    \"\"\"Alpha Vantage expects UTC like YYYYMMDDTHHMM.\"\"\"\n",
        "    return dt.astimezone(timezone.utc).strftime(\"%Y%m%dT%H%M\")\n",
        "\n",
        "def symbol_search_av(company: str, apikey: str, max_hits: int = 3) -> List[str]:\n",
        "    \"\"\"Return up to max_hits likely tickers for a company name using SYMBOL_SEARCH.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"SYMBOL_SEARCH\",\n",
        "        \"keywords\": company,\n",
        "        \"apikey\": apikey,\n",
        "    }\n",
        "    r = requests.get(AV_BASE, params=params, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    matches = data.get(\"bestMatches\", []) or []\n",
        "    tickers = []\n",
        "    for m in matches:\n",
        "        sym = (m.get(\"1. symbol\") or m.get(\"symbol\") or \"\").upper()\n",
        "        if sym and sym not in tickers:\n",
        "            tickers.append(sym)\n",
        "        if len(tickers) >= max_hits:\n",
        "            break\n",
        "    dprint(\"SYMBOL_SEARCH:\", company, \"->\", tickers)\n",
        "    return tickers\n",
        "\n",
        "# Alpha Vantage topic map (optional)\n",
        "AV_TOPICS = {\n",
        "    \"earnings\": \"earnings\",\n",
        "    \"ipo\": \"ipo\",\n",
        "    \"m&a\": \"mergers_and_acquisitions\",\n",
        "    \"merger\": \"mergers_and_acquisitions\",\n",
        "    \"acquisition\": \"mergers_and_acquisitions\",\n",
        "    \"macroeconomy\": \"economy_macro\",\n",
        "    \"inflation\": \"economy_monetary\",\n",
        "    \"interest rates\": \"economy_monetary\",\n",
        "    \"finance\": \"finance\",\n",
        "    \"technology\": \"technology\",\n",
        "    \"retail\": \"retail_wholesale\",\n",
        "    \"real estate\": \"real_estate\",\n",
        "    \"energy\": \"energy_transportation\",\n",
        "}\n",
        "\n",
        "def fetch_news_av(\n",
        "    apikey: str,\n",
        "    ticker: Optional[str] = None,\n",
        "    start_dt: Optional[datetime] = None,\n",
        "    end_dt: Optional[datetime] = None,\n",
        "    topics: Optional[List[str]] = None,\n",
        "    limit: int = 50,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fetch Alpha Vantage Market News & Sentiment.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"NEWS_SENTIMENT\",\n",
        "        \"apikey\": apikey,\n",
        "        \"limit\": min(limit, 1000),\n",
        "        \"sort\": \"LATEST\",\n",
        "    }\n",
        "    if ticker:\n",
        "        params[\"tickers\"] = ticker\n",
        "    if start_dt:\n",
        "        params[\"time_from\"] = av_time(start_dt)\n",
        "    if end_dt:\n",
        "        params[\"time_to\"] = av_time(end_dt)\n",
        "    if topics:\n",
        "        mapped = [AV_TOPICS[t] for t in topics if t in AV_TOPICS]\n",
        "        if mapped:\n",
        "            params[\"topics\"] = \",\".join(sorted(set(mapped)))\n",
        "\n",
        "    r = requests.get(AV_BASE, params=params, timeout=25)\n",
        "    r.raise_for_status()\n",
        "    js = r.json()\n",
        "\n",
        "    # Throttle / info messages come back as Note/Information/Error Message\n",
        "    if isinstance(js, dict) and any(k in js for k in (\"Note\", \"Information\", \"Error Message\")):\n",
        "        msg = js.get(\"Note\") or js.get(\"Information\") or js.get(\"Error Message\")\n",
        "        raise RuntimeError(f\"Alpha Vantage response: {msg}\")\n",
        "\n",
        "    feed = js.get(\"feed\", []) or []\n",
        "    dprint(f\"fetch_news_av(ticker={ticker}) -> {len(feed)} articles\")\n",
        "    return feed\n",
        "\n",
        "# --- Intent extraction + robust ticker detection -----------------------------\n",
        "\n",
        "UPPER_TICKER = re.compile(r\"\\b[A-Z]{1,5}(?:\\.[A-Z]{1,3})?\\b\")  # e.g., MSFT, AAPL, BRK.B\n",
        "\n",
        "def guess_tickers_from_text(text: str) -> List[str]:\n",
        "    \"\"\"Fast heuristic: pull likely tickers from uppercase tokens like MSFT, BRK.B, TSLA.\"\"\"\n",
        "    cands = [m.group(0).upper() for m in UPPER_TICKER.finditer(text)]\n",
        "    # Avoid obvious non-tickers\n",
        "    blacklist = {\"IPO\", \"EPS\", \"CEO\", \"AI\", \"CNN\", \"GDP\", \"US\", \"USA\", \"NSE\", \"BSE\"}\n",
        "    cands = [c for c in cands if c not in blacklist]\n",
        "    # Deduplicate preserving order\n",
        "    seen, out = set(), []\n",
        "    for c in cands:\n",
        "        if c not in seen:\n",
        "            seen.add(c); out.append(c)\n",
        "    dprint(\"guess_tickers_from_text:\", out)\n",
        "    return out\n",
        "\n",
        "def extract_intent_with_gemini(question: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Ask Gemini to return {companies, tickers, days_lookback, topics}.\n",
        "    If LLM returns non-JSON, fall back to heuristics.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You extract structured search intent for financial news questions. \"\n",
        "        \"Return ONLY JSON with keys: \"\n",
        "        \"{companies: [company names], tickers: [tickers if explicitly present], \"\n",
        "        \"days_lookback: integer (default 14), topics: [freeform words]}.\\n\\n\"\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"- If the user mentions dates like 'today', 'yesterday', 'last week', convert to an integer days_lookback.\\n\"\n",
        "        \"- If no timeframe given, use 14.\\n\"\n",
        "        \"- Companies should be plain names like 'Apple', 'Reliance Industries', 'Microsoft'.\\n\"\n",
        "        \"- Topics can be words like 'earnings', 'acquisition', 'antitrust', 'AI', 'supply chain'.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = client.models.generate_content(\n",
        "            model=GEMINI_MODEL,\n",
        "            contents=prompt,  # pass a string (not list/dicts)\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.2,\n",
        "                max_output_tokens=300,\n",
        "                top_p=0.9, # Added top_p\n",
        "                top_k=40,  # Added top_k\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "            ),\n",
        "        )\n",
        "        data = json.loads(resp.text)\n",
        "    except Exception as e:\n",
        "        dprint(\"Gemini intent parse failed:\", e)\n",
        "        data = {\"companies\": [], \"tickers\": [], \"days_lookback\": 14, \"topics\": []}\n",
        "\n",
        "    # Heuristic backfill: if no tickers from LLM, try raw text detection\n",
        "    if not data.get(\"tickers\"):\n",
        "        data[\"tickers\"] = guess_tickers_from_text(question)\n",
        "\n",
        "    if \"days_lookback\" not in data or not isinstance(data[\"days_lookback\"], int):\n",
        "        data[\"days_lookback\"] = 14\n",
        "\n",
        "    dprint(\"intent:\", data)\n",
        "    return data\n",
        "\n",
        "# --- Formatting + QA ---------------------------------------------------------\n",
        "\n",
        "def build_context_snippets(feeds: List[Dict[str, Any]], max_items: int = 12) -> str:\n",
        "    seen_urls = set()\n",
        "    lines, count = [], 0\n",
        "    for item in feeds:\n",
        "        url = (item.get(\"url\") or \"\").strip()\n",
        "        if not url or url in seen_urls:\n",
        "            continue\n",
        "        seen_urls.add(url)\n",
        "        count += 1\n",
        "        title = (item.get(\"title\") or \"\").strip()\n",
        "        src = (item.get(\"source\") or \"\").strip()\n",
        "        when = (item.get(\"time_published\") or \"\").strip()\n",
        "        summ = (item.get(\"summary\") or \"\").strip()\n",
        "        sent_label = (item.get(\"overall_sentiment_label\") or \"\").strip()\n",
        "        sent_score = item.get(\"overall_sentiment_score\", \"\")\n",
        "        lines.append(\n",
        "            f\"[{count}] {title} — {src} — {when}\\n{url}\\n\"\n",
        "            f\"Summary: {summ}\\nSentiment: {sent_label} ({sent_score})\\n\"\n",
        "        )\n",
        "        if count >= max_items:\n",
        "            break\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news; widen window if needed\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        return f\"News fetch error: {e}\"\n",
        "\n",
        "    if not all_news:\n",
        "        return \"I couldn’t find relevant business news for that query. Try adding a company name (e.g., 'Apple earnings this week').\"\n",
        "\n",
        "    context = build_context_snippets(all_news, max_items=12)\n",
        "    qa_prompt = (\n",
        "        \"You are a financial news analyst. Use only the articles below to answer the user's question.\\n\"\n",
        "        \"If you cite, refer to items like [1], [2]. Be concise, specific, and include recent dates.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Articles:\\n{context}\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=GEMINI_MODEL,\n",
        "        contents=qa_prompt,  # pass a string\n",
        "         config=types.GenerateContentConfig(\n",
        "            temperature=0.2,\n",
        "            max_output_tokens=900,\n",
        "            top_p=0.9, # Added top_p\n",
        "            top_k=40,  # Added top_k\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        ),\n",
        "    )\n",
        "    return resp.text\n",
        "\n",
        "# --- CLI ---------------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlouXCtmuO9j",
        "outputId": "ae342fcc-0d78-478c-9a54-b587ab4cdacc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: NVDA\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Nvidia (NVDA) recently reported Q2 earnings that exceeded analysts' forecasts, driven by AI data center construction and demand for its new Blackwell chip [2]. The company's performance contributed to a positive sentiment in consumer tech news from August 25-30, despite broader market wobbles [11].\n",
            "\n",
            "However, there are some notable points:\n",
            "*   **Customer Concentration:** Two undisclosed customers accounted for 39% of Nvidia's Q2 revenue, raising concerns about dependency [2].\n",
            "*   **Competition:** Alibaba Group is making moves to reduce its reliance on Nvidia [7]. While Nvidia is a prominent AI chip stock, some analysts suggest other companies might be better positioned for long-term gains in the AI infrastructure boom [4].\n",
            "*   **Long-Term Outlook:** Despite these concerns, some analysts predict Nvidia will \"soar\" over the next five years, seeing it as only beginning to tap into a multi-trillion-dollar opportunity [6]. It is also considered a high-quality growth stock for long-term investment [1].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stock trend analyasis chatbot use case"
      ],
      "metadata": {
        "id": "lJZkyauyubLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eZv-bVnug00",
        "outputId": "099bf003-3dbe-4ac8-a842-e885a5e8534b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: NVDA\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Nvidia (NVDA) recently reported Q2 earnings that exceeded analysts' forecasts, driven by AI data center construction and demand for its new Blackwell chip [2]. The company's performance contributed to a positive sentiment in consumer tech news from August 25-30, despite broader market wobbles [11].\n",
            "\n",
            "However, there are some notable points to consider:\n",
            "*   **Customer Concentration:** Two undisclosed customers accounted for 39% of Nvidia's Q2 revenue, raising concerns about dependency [2].\n",
            "*   **Competition:** Alibaba Group is making moves to reduce its reliance on Nvidia, indicating increasing competition in the chip market [7].\n",
            "*   **Alternative AI Investments:** While Nvidia often captures headlines, some analysts suggest other companies may be better positioned for long-term gains in the AI infrastructure boom [4].\n",
            "\n",
            "Despite these points, some analysts predict Nvidia will soar over the next five years, citing its early stage in tapping into a multi-trillion-dollar opportunity [6]. Nvidia is also considered a high-quality growth stock for long-term investment returns [1].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stock trend analysis based on data framework\n",
        "### Based on Alpha Vantage documentation, the TIME_SERIES_DAILY endpoint is used for historical daily stock data.\n",
        "#### The required parameters are:\n",
        "#### - function: TIME_SERIES_DAILY\n",
        "#### - symbol: The ticker symbol of the stock (e.g., NVDA, AAPL)\n",
        "#### - outputsize: compact (last 100 data points) or full (full historical data)\n",
        "#### - apikey: Your Alpha Vantage API key\n",
        "#### The response format is JSON and contains daily time series data with fields like '1. open', '2. high', '3. low', '4. close', '5. volume'."
      ],
      "metadata": {
        "id": "LOv6BUdQutDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def fetch_stock_data_av(\n",
        "    apikey: str,\n",
        "    ticker: str,\n",
        "    start_dt: Optional[datetime] = None,\n",
        "    end_dt: Optional[datetime] = None,\n",
        "    outputsize: str = 'compact' # 'compact' or 'full'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Fetch Alpha Vantage daily historical stock data.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"TIME_SERIES_DAILY\",\n",
        "        \"symbol\": ticker,\n",
        "        \"outputsize\": outputsize,\n",
        "        \"apikey\": apikey,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(AV_BASE, params=params, timeout=25)\n",
        "        r.raise_for_status()\n",
        "        js = r.json()\n",
        "\n",
        "        # Handle Alpha Vantage messages\n",
        "        if isinstance(js, dict) and any(k in js for k in (\"Note\", \"Information\", \"Error Message\")):\n",
        "            msg = js.get(\"Note\") or js.get(\"Information\") or js.get(\"Error Message\")\n",
        "            raise RuntimeError(f\"Alpha Vantage response: {msg}\")\n",
        "\n",
        "        # Extract daily time series data\n",
        "        time_series_data = js.get(\"Time Series (Daily)\", {}) or {}\n",
        "\n",
        "        if not time_series_data:\n",
        "            dprint(f\"No daily time series data found for ticker: {ticker}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Convert to pandas DataFrame\n",
        "        df = pd.DataFrame.from_dict(time_series_data, orient='index')\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        df = df.sort_index() # Ensure chronological order\n",
        "\n",
        "        # Filter by date range if provided\n",
        "        if start_dt:\n",
        "            df = df[df.index >= start_dt.replace(tzinfo=None)]\n",
        "        if end_dt:\n",
        "            df = df[df.index <= end_dt.replace(tzinfo=None)]\n",
        "\n",
        "        dprint(f\"fetch_stock_data_av(ticker={ticker}) -> {len(df)} rows\")\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise RuntimeError(f\"API request failed: {e}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error processing stock data: {e}\")\n"
      ],
      "metadata": {
        "id": "LNuY4zbbu6Wu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_stock_data(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Processes historical stock data to extract relevant statistics.\"\"\"\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Convert columns to numeric, forcing errors to NaN\n",
        "    for col in ['1. open', '2. high', '3. low', '4. close', '5. volume']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Drop rows with any NaN values after conversion\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Calculate daily price change and percentage change\n",
        "    df['daily_change'] = df['4. close'] - df['1. open']\n",
        "    df['daily_pct_change'] = (df['daily_change'] / df['1. open']) * 100\n",
        "\n",
        "    # Calculate moving averages\n",
        "    df['MA50'] = df['4. close'].rolling(window=50).mean()\n",
        "    df['MA200'] = df['4. close'].rolling(window=200).mean()\n",
        "\n",
        "    # Identify key values\n",
        "    latest_data = df.iloc[-1]\n",
        "    highest_close = df['4. close'].max()\n",
        "    lowest_close = df['4. close'].min()\n",
        "    date_highest_close = df['4. close'].idxmax()\n",
        "    date_lowest_close = df['4. close'].idxmin()\n",
        "\n",
        "    processed_info = {\n",
        "        \"latest_close\": latest_data['4. close'],\n",
        "        \"latest_open\": latest_data['1. open'],\n",
        "        \"latest_high\": latest_data['2. high'],\n",
        "        \"latest_low\": latest_data['3. low'],\n",
        "        \"latest_volume\": latest_data['5. volume'],\n",
        "        \"latest_daily_change\": latest_data['daily_change'],\n",
        "        \"latest_daily_pct_change\": latest_data['daily_pct_change'],\n",
        "        \"highest_close_price\": highest_close,\n",
        "        \"date_of_highest_close\": date_highest_close.strftime('%Y-%m-%d'),\n",
        "        \"lowest_close_price\": lowest_close,\n",
        "        \"date_of_lowest_close\": date_of_lowest_close.strftime('%Y-%m-%d'),\n",
        "        \"current_MA50\": latest_data['MA50'] if pd.notna(latest_data['MA50']) else None,\n",
        "        \"current_MA200\": latest_data['MA200'] if pd.notna(latest_data['MA200']) else None,\n",
        "        \"data_start_date\": df.index.min().strftime('%Y-%m-%d'),\n",
        "        \"data_end_date\": df.index.max().strftime('%Y-%m-%d'),\n",
        "        \"number_of_data_points\": len(df)\n",
        "    }\n",
        "\n",
        "    return processed_info"
      ],
      "metadata": {
        "id": "nm65FFIRvBv2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch stock data\n",
        "    stock_data = {}\n",
        "    if tickers:\n",
        "        # Assuming we only process the first resolved ticker for stock data for simplicity in this step\n",
        "        try:\n",
        "            df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "            stock_data = process_stock_data(df)\n",
        "        except RuntimeError as e:\n",
        "            return f\"Stock data fetch or processing error: {e}\"\n",
        "    else:\n",
        "         return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "\n",
        "    if not stock_data:\n",
        "        return \"I couldn’t find historical stock data for that query. Try specifying a company name or ticker.\"\n",
        "\n",
        "    # Format the numerical data for the prompt\n",
        "    data_context = json.dumps(stock_data, indent=2)\n",
        "\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst answering questions based *only* on the provided historical stock data. \"\n",
        "        \"Do NOT use any external knowledge or refer to news articles. \"\n",
        "        \"Analyze the numerical data below to answer the user's question. \"\n",
        "        \"Include specific values and date ranges mentioned in the data where relevant. \"\n",
        "        \"If the requested information cannot be found in the data, state that clearly.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=GEMINI_MODEL,\n",
        "        contents=qa_prompt,  # pass a string\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.2,\n",
        "            max_output_tokens=900,\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        ),\n",
        "    )\n",
        "    return resp.text"
      ],
      "metadata": {
        "id": "3Ojaer5_vFDl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# --- Gemini client ---\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "GEMINI_MODEL = \"gemini-2.5-flash\"\n",
        "AV_BASE = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "client = genai.Client()  # reads GOOGLE_API_KEY from env\n",
        "DEBUG = os.environ.get(\"DEBUG_NEWS_BOT\") == \"1\"\n",
        "\n",
        "def dprint(*args):\n",
        "    if DEBUG:\n",
        "        print(\"[debug]\", *args)\n",
        "\n",
        "# --- Utilities ---------------------------------------------------------------\n",
        "\n",
        "def av_time(dt: datetime) -> str:\n",
        "    \"\"\"Alpha Vantage expects UTC like YYYYMMDDTHHMM.\"\"\"\n",
        "    return dt.astimezone(timezone.utc).strftime(\"%Y%m%dT%H%M\")\n",
        "\n",
        "def symbol_search_av(company: str, apikey: str, max_hits: int = 3) -> List[str]:\n",
        "    \"\"\"Return up to max_hits likely tickers for a company name using SYMBOL_SEARCH.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"SYMBOL_SEARCH\",\n",
        "        \"keywords\": company,\n",
        "        \"apikey\": apikey,\n",
        "    }\n",
        "    r = requests.get(AV_BASE, params=params, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    matches = data.get(\"bestMatches\", []) or []\n",
        "    tickers = []\n",
        "    for m in matches:\n",
        "        sym = (m.get(\"1. symbol\") or m.get(\"symbol\") or \"\").upper()\n",
        "        if sym and sym not in tickers:\n",
        "            tickers.append(sym)\n",
        "        if len(tickers) >= max_hits:\n",
        "            break\n",
        "    dprint(\"SYMBOL_SEARCH:\", company, \"->\", tickers)\n",
        "    return tickers\n",
        "\n",
        "# Alpha Vantage topic map (optional)\n",
        "AV_TOPICS = {\n",
        "    \"earnings\": \"earnings\",\n",
        "    \"ipo\": \"ipo\",\n",
        "    \"m&a\": \"mergers_and_acquisitions\",\n",
        "    \"merger\": \"mergers_and_acquisitions\",\n",
        "    \"acquisition\": \"mergers_and_acquisitions\",\n",
        "    \"macroeconomy\": \"economy_macro\",\n",
        "    \"inflation\": \"economy_monetary\",\n",
        "    \"interest rates\": \"economy_monetary\",\n",
        "    \"finance\": \"finance\",\n",
        "    \"technology\": \"technology\",\n",
        "    \"retail\": \"retail_wholesale\",\n",
        "    \"real estate\": \"real_estate\",\n",
        "    \"energy\": \"energy_transportation\",\n",
        "}\n",
        "\n",
        "def fetch_news_av(\n",
        "    apikey: str,\n",
        "    ticker: Optional[str] = None,\n",
        "    start_dt: Optional[datetime] = None,\n",
        "    end_dt: Optional[datetime] = None,\n",
        "    topics: Optional[List[str]] = None,\n",
        "    limit: int = 50,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fetch Alpha Vantage Market News & Sentiment.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"NEWS_SENTIMENT\",\n",
        "        \"apikey\": apikey,\n",
        "        \"limit\": min(limit, 1000),\n",
        "        \"sort\": \"LATEST\",\n",
        "    }\n",
        "    if ticker:\n",
        "        params[\"tickers\"] = ticker\n",
        "    if start_dt:\n",
        "        params[\"time_from\"] = av_time(start_dt)\n",
        "    if end_dt:\n",
        "        params[\"time_to\"] = av_time(end_dt)\n",
        "    if topics:\n",
        "        mapped = [AV_TOPICS[t] for t in topics if t in AV_TOPICS]\n",
        "        if mapped:\n",
        "            params[\"topics\"] = \",\".join(sorted(set(mapped)))\n",
        "\n",
        "    r = requests.get(AV_BASE, params=params, timeout=25)\n",
        "    r.raise_for_status()\n",
        "    js = r.json()\n",
        "\n",
        "    # Throttle / info messages come back as Note/Information/Error Message\n",
        "    if isinstance(js, dict) and any(k in js for k in (\"Note\", \"Information\", \"Error Message\")):\n",
        "        msg = js.get(\"Note\") or js.get(\"Information\") or js.get(\"Error Message\")\n",
        "        raise RuntimeError(f\"Alpha Vantage response: {msg}\")\n",
        "\n",
        "    feed = js.get(\"feed\", []) or []\n",
        "    dprint(f\"fetch_news_av(ticker={ticker}) -> {len(feed)} articles\")\n",
        "    return feed\n",
        "\n",
        "# --- Intent extraction + robust ticker detection -----------------------------\n",
        "\n",
        "UPPER_TICKER = re.compile(r\"\\b[A-Z]{1,5}(?:\\.[A-Z]{1,3})?\\b\")  # e.g., MSFT, AAPL, BRK.B\n",
        "\n",
        "def guess_tickers_from_text(text: str) -> List[str]:\n",
        "    \"\"\"Fast heuristic: pull likely tickers from uppercase tokens like MSFT, BRK.B, TSLA.\"\"\"\n",
        "    cands = [m.group(0).upper() for m in UPPER_TICKER.finditer(text)]\n",
        "    # Avoid obvious non-tickers\n",
        "    blacklist = {\"IPO\", \"EPS\", \"CEO\", \"AI\", \"CNN\", \"GDP\", \"US\", \"USA\", \"NSE\", \"BSE\"}\n",
        "    cands = [c for c in cands if c not in blacklist]\n",
        "    # Deduplicate preserving order\n",
        "    seen, out = set(), []\n",
        "    for c in cands:\n",
        "        if c not in seen:\n",
        "            seen.add(c); out.append(c)\n",
        "    dprint(\"guess_tickers_from_text:\", out)\n",
        "    return out\n",
        "\n",
        "def extract_intent_with_gemini(question: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Ask Gemini to return {companies, tickers, days_lookback, topics}.\n",
        "    If LLM returns non-JSON, fall back to heuristics.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You extract structured search intent for financial news questions. \"\n",
        "        \"Return ONLY JSON with keys: \"\n",
        "        \"{companies: [company names], tickers: [tickers if explicitly present], \"\n",
        "        \"days_lookback: integer (default 14), topics: [freeform words]}.\\n\\n\"\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"- If the user mentions dates like 'today', 'yesterday', 'last week', convert to an integer days_lookback.\\n\"\n",
        "        \"- If no timeframe given, use 14.\\n\"\n",
        "        \"- Companies should be plain names like 'Apple', 'Reliance Industries', 'Microsoft'.\\n\"\n",
        "        \"- Topics can be words like 'earnings', 'acquisition', 'antitrust', 'AI', 'supply chain'.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = client.models.generate_content(\n",
        "            model=GEMINI_MODEL,\n",
        "            contents=prompt,  # pass a string (not list/dicts)\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.2,\n",
        "                max_output_tokens=300,\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "            ),\n",
        "        )\n",
        "        data = json.loads(resp.text)\n",
        "    except Exception as e:\n",
        "        dprint(\"Gemini intent parse failed:\", e)\n",
        "        data = {\"companies\": [], \"tickers\": [], \"days_lookback\": 14, \"topics\": []}\n",
        "\n",
        "    # Heuristic backfill: if no tickers from LLM, try raw text detection\n",
        "    if not data.get(\"tickers\"):\n",
        "        data[\"tickers\"] = guess_tickers_from_text(question)\n",
        "\n",
        "    if \"days_lookback\" not in data or not isinstance(data[\"days_lookback\"], int):\n",
        "        data[\"days_lookback\"] = 14\n",
        "\n",
        "    dprint(\"intent:\", data)\n",
        "    return data\n",
        "\n",
        "# --- Formatting + QA ---------------------------------------------------------\n",
        "\n",
        "def build_context_snippets(feeds: List[Dict[str, Any]], max_items: int = 12) -> str:\n",
        "    seen_urls = set()\n",
        "    lines, count = [], 0\n",
        "    for item in feeds:\n",
        "        url = (item.get(\"url\") or \"\").strip()\n",
        "        if not url or url in seen_urls:\n",
        "            continue\n",
        "        seen_urls.add(url)\n",
        "        count += 1\n",
        "        title = (item.get(\"title\") or \"\").strip()\n",
        "        src = (item.get(\"source\") or \"\").strip()\n",
        "        when = (item.get(\"time_published\") or \"\").strip()\n",
        "        summ = (item.get(\"summary\") or \"\").strip()\n",
        "        sent_label = (item.get(\"overall_sentiment_label\") or \"\").strip()\n",
        "        sent_score = item.get(\"overall_sentiment_score\", \"\")\n",
        "        lines.append(\n",
        "            f\"[{count}] {title} — {src} — {when}\\n{url}\\n\"\n",
        "            f\"Summary: {summ}\\nSentiment: {sent_label} ({sent_score})\\n\"\n",
        "        )\n",
        "        if count >= max_items:\n",
        "            break\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_stock_data_av(\n",
        "    apikey: str,\n",
        "    ticker: str,\n",
        "    start_dt: Optional[datetime] = None,\n",
        "    end_dt: Optional[datetime] = None,\n",
        "    outputsize: str = 'compact' # 'compact' or 'full'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Fetch Alpha Vantage daily historical stock data.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"TIME_SERIES_DAILY\",\n",
        "        \"symbol\": ticker,\n",
        "        \"outputsize\": outputsize,\n",
        "        \"apikey\": apikey,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(AV_BASE, params=params, timeout=25)\n",
        "        r.raise_for_status()\n",
        "        js = r.json()\n",
        "\n",
        "        # Handle Alpha Vantage messages\n",
        "        if isinstance(js, dict) and any(k in js for k in (\"Note\", \"Information\", \"Error Message\")):\n",
        "            msg = js.get(\"Note\") or js.get(\"Information\") or js.get(\"Error Message\")\n",
        "            raise RuntimeError(f\"Alpha Vantage response: {msg}\")\n",
        "\n",
        "        # Extract daily time series data\n",
        "        time_series_data = js.get(\"Time Series (Daily)\", {}) or {}\n",
        "\n",
        "        if not time_series_data:\n",
        "            dprint(f\"No daily time series data found for ticker: {ticker}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Convert to pandas DataFrame\n",
        "        df = pd.DataFrame.from_dict(time_series_data, orient='index')\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        df = df.sort_index() # Ensure chronological order\n",
        "\n",
        "        # Filter by date range if provided\n",
        "        if start_dt:\n",
        "            df = df[df.index >= start_dt.replace(tzinfo=None)]\n",
        "        if end_dt:\n",
        "            df = df[df.index <= end_dt.replace(tzinfo=None)]\n",
        "\n",
        "        dprint(f\"fetch_stock_data_av(ticker={ticker}) -> {len(df)} rows\")\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise RuntimeError(f\"API request failed: {e}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error processing stock data: {e}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def process_stock_data(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Processes historical stock data to extract relevant statistics.\"\"\"\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Convert columns to numeric, forcing errors to NaN\n",
        "    for col in ['1. open', '2. high', '3. low', '4. close', '5. volume']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Drop rows with any NaN values after conversion\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Calculate daily price change and percentage change\n",
        "    df['daily_change'] = df['4. close'] - df['1. open']\n",
        "    df['daily_pct_change'] = (df['daily_change'] / df['1. open']) * 100\n",
        "\n",
        "    # Calculate moving averages\n",
        "    df['MA50'] = df['4. close'].rolling(window=50).mean()\n",
        "    df['MA200'] = df['4. close'].rolling(window=200).mean()\n",
        "\n",
        "    # Identify key values\n",
        "    latest_data = df.iloc[-1]\n",
        "    highest_close = df['4. close'].max()\n",
        "    lowest_close = df['4. close'].min()\n",
        "    date_highest_close = df['4. close'].idxmax()\n",
        "    date_lowest_close_val = df['4. close'].idxmin() # Corrected variable name\n",
        "\n",
        "    processed_info = {\n",
        "        \"latest_close\": latest_data['4. close'],\n",
        "        \"latest_open\": latest_data['1. open'],\n",
        "        \"latest_high\": latest_data['2. high'],\n",
        "        \"latest_low\": latest_data['3. low'],\n",
        "        \"latest_volume\": latest_data['5. volume'],\n",
        "        \"latest_daily_change\": latest_data['daily_change'],\n",
        "        \"latest_daily_pct_change\": latest_data['daily_pct_change'],\n",
        "        \"highest_close_price\": highest_close,\n",
        "        \"date_of_highest_close\": date_highest_close.strftime('%Y-%m-%d'),\n",
        "        \"lowest_close_price\": lowest_close,\n",
        "        \"date_of_lowest_close\": date_lowest_close_val.strftime('%Y-%m-%d'), # Using the corrected variable name\n",
        "        \"current_MA50\": latest_data['MA50'] if pd.notna(latest_data['MA50']) else None,\n",
        "        \"current_MA200\": latest_data['MA200'] if pd.notna(latest_data['MA200']) else None,\n",
        "        \"data_start_date\": df.index.min().strftime('%Y-%m-%d'),\n",
        "        \"data_end_date\": df.index.max().strftime('%Y-%m-%d'),\n",
        "        \"number_of_data_points\": len(df)\n",
        "    }\n",
        "\n",
        "    return processed_info\n",
        "\n",
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    # topics are not used for stock data, but keeping the extraction for potential future use\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch and process stock data\n",
        "    stock_data = {}\n",
        "    if tickers:\n",
        "        # Process only the first resolved ticker for simplicity in this step\n",
        "        try:\n",
        "            # Fetch full data to allow for MA calculations over longer periods\n",
        "            df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "            stock_data = process_stock_data(df)\n",
        "        except RuntimeError as e:\n",
        "            return f\"Stock data fetch or processing error: {e}\"\n",
        "    else:\n",
        "         return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "\n",
        "    if not stock_data or not stock_data.get(\"number_of_data_points\"):\n",
        "        return \"I couldn’t find sufficient historical stock data for that query. Try specifying a company name or ticker or a different date range.\"\n",
        "\n",
        "    # Format the numerical data for the prompt\n",
        "    data_context = json.dumps(stock_data, indent=2)\n",
        "\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst answering questions based *only* on the provided historical stock data. \"\n",
        "        \"Do NOT use any external knowledge or refer to news articles. \"\n",
        "        \"Analyze the numerical data below to answer the user's question. \"\n",
        "        \"Include specific values and date ranges mentioned in the data where relevant. \"\n",
        "        \"If the requested information cannot be found in the data, state that clearly.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=GEMINI_MODEL,\n",
        "        contents=qa_prompt,  # pass a string\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.2,\n",
        "            max_output_tokens=900,\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        ),\n",
        "    )\n",
        "    return resp.text\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa4i-8IXvJIf",
        "outputId": "d2965458-5ae6-455e-fc81-11b64c9e36d1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: NVDA\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Based on the provided historical stock data for NVDA:\n",
            "\n",
            "On the latest trading day (2025-08-29), NVDA closed at 174.18, opened at 178.11, reached a high of 178.15, and a low of 173.145. The daily change was -3.93, representing a -2.2065% decrease, with a volume of 243,257,873.0.\n",
            "\n",
            "Over the period from 2025-08-18 to 2025-08-29 (10 data points):\n",
            "*   The highest close price was 182.01 on 2025-08-18.\n",
            "*   The lowest close price was 174.18 on 2025-08-29.\n",
            "\n",
            "The 50-day Moving Average (MA50) and 200-day Moving Average (MA200) are not available in the provided data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify relevant api endpoint\n",
        "\n",
        "### Subtask:\n",
        "#### - Determine the appropriate Alpha Vantage API endpoint for historical stock data (e.g., TIME_SERIES_DAILY).\n",
        "\n",
        "### **Reasoning**:\n",
        "#### - I need to determine the Alpha Vantage API endpoint for historical stock data. I will consult the Alpha Vantage API documentation to find the correct endpoint and its parameters.\n",
        "\n",
        "### Based on Alpha Vantage documentation, the TIME_SERIES_DAILY endpoint is used for historical daily stock data.\n",
        "#### - The required parameters are:\n",
        "#### - function: TIME_SERIES_DAILY\n",
        "#### - symbol: The ticker symbol of the stock (e.g., NVDA, AAPL)\n",
        "#### - outputsize: compact (last 100 data points) or full (full historical data)\n",
        "#### - apikey: Your Alpha Vantage API key\n",
        "#### - The response format is JSON and contains daily time series data with fields like '1. open', '2. high', '3. low', '4. close', '5. volume'.\n",
        "\n",
        "### Task\n",
        "#### - Create Python code using the Alpha Vantage API to fetch historical stock trend data and use the existing AI model to answer questions about the stock trends.\n",
        "\n",
        "### Test the updated function\n",
        "\n",
        "#### - Subtask:\n",
        "#### - Run the modified Q&A function with a sample question about stock trends to verify it uses the numerical data and provides relevant answers.\n",
        "\n",
        "### **Reasoning**:\n",
        "#### - Call the modified `answer_company_news_question` function with a sample question about stock trends to verify its functionality.\n"
      ],
      "metadata": {
        "id": "YsyH2zdDvTML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the stock trend for NVDA over the last 30 days?\"\n",
        "print(f\"Ask a company news question: {question}\\n\")\n",
        "print(\"Thinking...\\n\")\n",
        "answer = answer_company_news_question(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAxvy20EwAxZ",
        "outputId": "06399f2e-f772-433b-e30b-e39d72486a8c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: What is the stock trend for NVDA over the last 30 days?\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Based on the provided historical stock data, I cannot determine the stock trend for NVDA over the last 30 days. The available data only covers the period from 2025-08-18 to 2025-08-29, which is less than 30 days.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reasoning**:\n",
        "#### - The previous code failed because of a `NameError`. The variable `date_of_lowest_close` was not defined in the `process_stock_data` function. I need to fix this error in the `process_stock_data` function and re-run the function call.\n"
      ],
      "metadata": {
        "id": "8L5Jm-dDwFnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_stock_data(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Processes historical stock data to extract relevant statistics.\"\"\"\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Convert columns to numeric, forcing errors to NaN\n",
        "    for col in ['1. open', '2. high', '3. low', '4. close', '5. volume']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Drop rows with any NaN values after conversion\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Calculate daily price change and percentage change\n",
        "    df['daily_change'] = df['4. close'] - df['1. open']\n",
        "    df['daily_pct_change'] = (df['daily_change'] / df['1. open']) * 100\n",
        "\n",
        "    # Calculate moving averages\n",
        "    df['MA50'] = df['4. close'].rolling(window=50).mean()\n",
        "    df['MA200'] = df['4. close'].rolling(window=200).mean()\n",
        "\n",
        "    # Identify key values\n",
        "    latest_data = df.iloc[-1]\n",
        "    highest_close = df['4. close'].max()\n",
        "    lowest_close = df['4. close'].min()\n",
        "    date_highest_close = df['4. close'].idxmax()\n",
        "    date_lowest_close_val = df['4. close'].idxmin() # Corrected variable name\n",
        "\n",
        "    processed_info = {\n",
        "        \"latest_close\": latest_data['4. close'],\n",
        "        \"latest_open\": latest_data['1. open'],\n",
        "        \"latest_high\": latest_data['2. high'],\n",
        "        \"latest_low\": latest_data['3. low'],\n",
        "        \"latest_volume\": latest_data['5. volume'],\n",
        "        \"latest_daily_change\": latest_data['daily_change'],\n",
        "        \"latest_daily_pct_change\": latest_data['daily_pct_change'],\n",
        "        \"highest_close_price\": highest_close,\n",
        "        \"date_of_highest_close\": date_highest_close.strftime('%Y-%m-%d'),\n",
        "        \"lowest_close_price\": lowest_close,\n",
        "        \"date_of_lowest_close\": date_lowest_close_val.strftime('%Y-%m-%d'), # Using the corrected variable name\n",
        "        \"current_MA50\": latest_data['MA50'] if pd.notna(latest_data['MA50']) else None,\n",
        "        \"current_MA200\": latest_data['MA200'] if pd.notna(latest_data['MA200']) else None,\n",
        "        \"data_start_date\": df.index.min().strftime('%Y-%m-%d'),\n",
        "        \"data_end_date\": df.index.max().strftime('%Y-%m-%d'),\n",
        "        \"number_of_data_points\": len(df)\n",
        "    }\n",
        "\n",
        "    return processed_info\n",
        "\n",
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    # topics are not used for stock data, but keeping the extraction for potential future use\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch and process stock data\n",
        "    stock_data = {}\n",
        "    if tickers:\n",
        "        # Process only the first resolved ticker for simplicity in this step\n",
        "        try:\n",
        "            # Fetch full data to allow for MA calculations over longer periods\n",
        "            df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "            stock_data = process_stock_data(df)\n",
        "        except RuntimeError as e:\n",
        "            return f\"Stock data fetch or processing error: {e}\"\n",
        "    else:\n",
        "         return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "\n",
        "    if not stock_data or not stock_data.get(\"number_of_data_points\"):\n",
        "        return \"I couldn’t find sufficient historical stock data for that query. Try specifying a company name or ticker or a different date range.\"\n",
        "\n",
        "    # Format the numerical data for the prompt\n",
        "    data_context = json.dumps(stock_data, indent=2)\n",
        "\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst answering questions based *only* on the provided historical stock data. \"\n",
        "        \"Do NOT use any external knowledge or refer to news articles. \"\n",
        "        \"Analyze the numerical data below to answer the user's question. \"\n",
        "        \"Include specific values and date ranges mentioned in the data where relevant. \"\n",
        "        \"If the requested information cannot be found in the data, state that clearly.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=GEMINI_MODEL,\n",
        "        contents=qa_prompt,  # pass a string\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.2,\n",
        "            max_output_tokens=900,\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        ),\n",
        "    )\n",
        "    return resp.text\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4KxiojevcRY",
        "outputId": "3b3bf835-f7f8-48eb-b4e3-6e066ef745f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: NVDA\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Based on the provided historical stock data for NVDA:\n",
            "\n",
            "On the latest trading day (2025-08-29), NVDA closed at 174.18, opened at 178.11, reached a high of 178.15, and a low of 173.145. The daily change was -3.93, representing a -2.2065% decrease, with a volume of 243,257,873.0.\n",
            "\n",
            "Over the period from 2025-08-18 to 2025-08-29 (10 data points):\n",
            "*   The highest close price was 182.01 on 2025-08-18.\n",
            "*   The lowest close price was 174.18 on 2025-08-29.\n",
            "\n",
            "The 50-day Moving Average (MA50) and 200-day Moving Average (MA200) are not available in the provided data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reasoning**:\n",
        "#### - The previous attempt failed to identify a ticker from the question. I will call the `answer_company_news_question` function again, this time providing a clearer question that includes the ticker symbol, to ensure a ticker is identified and the stock data fetching and processing is attempted."
      ],
      "metadata": {
        "id": "BLgwx6kOwSwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the stock trend for NVDA over the last 30 days?\"\n",
        "print(f\"Ask a company news question: {question}\\n\")\n",
        "print(\"Thinking...\\n\")\n",
        "answer = answer_company_news_question(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiM8cnctwYwn",
        "outputId": "907a51e9-da52-4779-eac0-4599d6a23223"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: What is the stock trend for NVDA over the last 30 days?\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Based on the provided historical stock data, I cannot determine the stock trend for NVDA over the last 30 days. The available data only covers the period from 2025-08-18 to 2025-08-29, which is less than 30 days.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A `NameError` was identified and corrected in the `process_stock_data` function related to handling the date of the lowest close price.\n",
        "*   The system successfully extracted the ticker symbol ex. \"NVDA\" or \"AAPL\" when it was explicitly provided in the user's question.\n",
        "*   The AI model utilized the provided numerical stock data to answer the question about the stock trend.\n",
        "*   The AI's response acknowledged the limitation of the available data range (10 days between \"2025-08-18\" and \"2025-08-29\") and based its analysis solely on this period, including details like the highest and lowest closing prices within this timeframe.\n",
        "*   The AI's response did not incorporate any information from news articles, confirming it adhered to the constraint of using only the provided numerical data.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Implement more robust ticker symbol identification, potentially using a combination of company name search and direct ticker matching, to handle queries where the ticker is not explicitly provided.\n",
        "*   Enhance the AI's ability to summarize trends over limited data periods when the requested range is not fully available.\n"
      ],
      "metadata": {
        "id": "Rrb9h5wpwd8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task\n",
        "#### - Create a Python code using an API endpoint to pull stock trend data and combine it with news-based answers from an existing AI to provide a unified response to a user's question about a stock trend. The response should clearly show: a. answer based on news, b. answers based on numerical data, and c. combined answer.\n",
        "\n",
        "### Refactor data fetching\n",
        "\n",
        "### Subtask:\n",
        "#### - Modify the `answer_company_news_question` function to fetch both news articles and historical stock data for the identified ticker(s).\n",
        "\n",
        "### **Reasoning**:\n",
        "#### - I need to modify the `answer_company_news_question` function to fetch both news articles and historical stock data. I will add calls to `fetch_news_av` and `fetch_stock_data_av` within the function, handling potential errors.\n",
        "\n"
      ],
      "metadata": {
        "id": "vGHhCOR5whTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    if not tickers:\n",
        "        return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news articles\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no news articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback for news\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        return f\"News fetch error: {e}\"\n",
        "\n",
        "    # Fetch and process stock data (using the first resolved ticker)\n",
        "    stock_data = {}\n",
        "    try:\n",
        "        # Fetch full data to allow for MA calculations over longer periods\n",
        "        df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "        stock_data = process_stock_data(df)\n",
        "    except RuntimeError as e:\n",
        "        # This is not a fatal error, just report it and continue without stock data\n",
        "        dprint(f\"Stock data fetch or processing error: {e}\")\n",
        "        stock_data = {} # Ensure stock_data is empty if fetching fails\n",
        "\n",
        "\n",
        "    # Now, decide how to answer based on available data (news, stock, or both)\n",
        "    # This part will be modified in subsequent subtasks to combine answers.\n",
        "    # For now, we'll just indicate what data was found.\n",
        "\n",
        "    response_parts = []\n",
        "    if all_news:\n",
        "        response_parts.append(f\"Found {len(all_news)} relevant news articles.\")\n",
        "    else:\n",
        "        response_parts.append(\"No relevant news articles found.\")\n",
        "\n",
        "    if stock_data and stock_data.get(\"number_of_data_points\"):\n",
        "         response_parts.append(f\"Found {stock_data.get('number_of_data_points')} days of historical stock data.\")\n",
        "    else:\n",
        "         response_parts.append(\"No sufficient historical stock data found.\")\n",
        "\n",
        "    # Placeholder for the combined answer generation in a later step\n",
        "    # For now, just return the data availability status.\n",
        "    return \"\\n\".join(response_parts)\n",
        "\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr22d9wKwvxO",
        "outputId": "afa2500a-b0ca-4fb6-80dd-54911dfb1a3e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: NVDA\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Found 50 relevant news articles.\n",
            "Found 10 days of historical stock data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process both data types; text, data\n",
        "\n",
        "### Subtask:\n",
        "#### - Ensure both news articles and stock data are processed appropriately using the existing `build_context_snippets` and `process_stock_data` functions.\n",
        "\n",
        "### **Reasoning**:\n",
        "#### - Call the `build_context_snippets` function with the fetched news articles to generate a formatted string of news snippets and ensure the `process_stock_data` function is called with the fetched stock data, storing the results in variables for later use.\n"
      ],
      "metadata": {
        "id": "yiRmIvPqw0vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    if not tickers:\n",
        "        return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news articles\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no news articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback for news\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        # This is not a fatal error, just report it and continue without news data\n",
        "        dprint(f\"News fetch error: {e}\")\n",
        "        all_news = [] # Ensure all_news is empty if fetching fails\n",
        "\n",
        "    # Process news data\n",
        "    news_context = build_context_snippets(all_news)\n",
        "\n",
        "    # Fetch and process stock data (using the first resolved ticker)\n",
        "    stock_data = {}\n",
        "    try:\n",
        "        # Fetch full data to allow for MA calculations over longer periods\n",
        "        df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "        stock_data = process_stock_data(df)\n",
        "    except RuntimeError as e:\n",
        "        # This is not a fatal error, just report it and continue without stock data\n",
        "        dprint(f\"Stock data fetch or processing error: {e}\")\n",
        "        stock_data = {} # Ensure stock_data is empty if fetching fails\n",
        "\n",
        "    # Now, decide how to answer based on available data (news, stock, or both)\n",
        "    # This part will be modified in subsequent subtasks to combine answers.\n",
        "    # For now, we'll just indicate what data was found and processed.\n",
        "\n",
        "    response_parts = []\n",
        "    if news_context:\n",
        "        response_parts.append(\"News articles processed.\")\n",
        "    else:\n",
        "        response_parts.append(\"No news articles processed.\")\n",
        "\n",
        "    if stock_data and stock_data.get(\"number_of_data_points\"):\n",
        "         response_parts.append(\"Historical stock data processed.\")\n",
        "    else:\n",
        "         response_parts.append(\"No historical stock data processed.\")\n",
        "\n",
        "    # Placeholder for the combined answer generation in a later step\n",
        "    # For now, just return the data availability status.\n",
        "    return \"\\n\".join(response_parts)\n",
        "\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmaEtOwvw_PD",
        "outputId": "4f1798bd-b5de-4a32-c341-b43698cadcdf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: NVDA\n",
            "\n",
            "Thinking...\n",
            "\n",
            "News articles processed.\n",
            "Historical stock data processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update ai prompt for combined context\n",
        "\n",
        "### Subtask:\n",
        "#### - Create a new AI prompt that includes both the formatted news snippets and the processed numerical stock data, instructing Gemini to provide a combined answer as well as separate news-based and data-based insights."
      ],
      "metadata": {
        "id": "SasFPP2PxFod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    if not tickers:\n",
        "        return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news articles\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no news articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback for news\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        # This is not a fatal error, just report it and continue without news data\n",
        "        dprint(f\"News fetch error: {e}\")\n",
        "        all_news = [] # Ensure all_news is empty if fetching fails\n",
        "\n",
        "    # Process news data\n",
        "    news_context = build_context_snippets(all_news)\n",
        "\n",
        "    # Fetch and process stock data (using the first resolved ticker)\n",
        "    stock_data = {}\n",
        "    try:\n",
        "        # Fetch full data to allow for MA calculations over longer periods\n",
        "        df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "        stock_data = process_stock_data(df)\n",
        "    except RuntimeError as e:\n",
        "        # This is not a fatal error, just report it and continue without stock data\n",
        "        dprint(f\"Stock data fetch or processing error: {e}\")\n",
        "        stock_data = {} # Ensure stock_data is empty if fetching fails\n",
        "\n",
        "    # Format the numerical data for the prompt\n",
        "    data_context = json.dumps(stock_data, indent=2) if stock_data else \"No historical stock data available.\"\n",
        "\n",
        "    # Construct the combined prompt for Gemini\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst. Answer the user's question based *only* on the provided information (news articles and historical stock data).\\n\"\n",
        "        \"Do NOT use any external knowledge.\\n\"\n",
        "        \"Provide three distinct answers:\\n\"\n",
        "        \"1. News-Based Answer: Based *only* on the provided news articles. Cite articles using [citation number].\\n\"\n",
        "        \"2. Data-Based Answer: Based *only* on the provided historical stock data. Include specific values and date ranges mentioned in the data where relevant.\\n\"\n",
        "        \"3. Combined Answer: Synthesize insights from *both* the news articles and the historical stock data to provide a comprehensive answer to the user's question.\\n\\n\"\n",
        "        \"If the requested information cannot be found in either the news or the data, state that clearly in the relevant section.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Articles:\\n{news_context if news_context else 'No news articles provided.'}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=GEMINI_MODEL,\n",
        "        contents=qa_prompt,  # pass a string\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.2,\n",
        "            max_output_tokens=900,\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        ),\n",
        "    )\n",
        "    return resp.text\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dqZDQ5ZxJ9s",
        "outputId": "306ea551-2315-4213-81ef-eb502f6f16d9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: NVDA\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Here are three distinct answers regarding NVDA, based solely on the provided information:\n",
            "\n",
            "**1. News-Based Answer:**\n",
            "Nvidia (NVDA) recently reported second-quarter earnings that exceeded analysts' forecasts, driven by AI data center construction and demand for its new Blackwell chip [2]. The company is seen as a high-quality growth stock that can strengthen long-term investment returns [1] and is predicted to soar over the next five years as it begins to tap into a multitrillion-dollar opportunity [6]. Nvidia's performance contributed to a \"shining\" period for the company, even as broader stocks experienced wobbles due to hotter core PCE data [11]. However, there are concerns about Nvidia's heavy reliance on two undisclosed customers, who collectively drove 39% of its Q2 revenue [2]. Additionally, while Nvidia captures headlines, some believe another critical enabler of the AI infrastructure boom may be better positioned for long-term gains [4]. Alibaba Group is also making moves to reduce its reliance on Nvidia Corporation [7].\n",
            "\n",
            "**2. Data-Based Answer:**\n",
            "Nvidia's latest closing price was $174.18. On that day, the stock opened at $178.11, reached a high of $178.15, and a low of $173.145. It experienced a daily change of -$3.93, representing a -2.2065% decrease, with a volume of 243,257,873.0 shares. Over the provided data range from August 18, 2025, to August 29, 2025, Nvidia's highest closing price was $182.01 on August 18, 2025. Its lowest closing price during this period was $174.18 on August 29, 2025.\n",
            "\n",
            "**3. Combined Answer:**\n",
            "Nvidia (NVDA) has recently demonstrated strong performance, with its second-quarter earnings exceeding forecasts, largely due to demand for its Blackwell chip and AI data center construction [2]. This positive news aligns with predictions that Nvidia is a growth stock poised to soar over the next five years, tapping into a multitrillion-dollar opportunity [1, 6]. However, the historical stock data shows that despite these positive news items, Nvidia's latest closing price of $174.18 on August 29, 2025, represents its lowest closing price within the provided data range (August 18-29, 2025), and a daily decrease of 2.2065% from its opening price [Data]. This suggests that while the long-term outlook from news articles is bullish, the stock has experienced recent downward pressure. A potential factor contributing to this could be the concern raised in the news about Nvidia's significant dependency on two undisclosed customers for 39% of its Q2 revenue [2], or the fact that other companies like Alibaba are seeking to reduce their reliance on Nvidia [7]. Furthermore, some analysts suggest that while Nvidia is prominent, another AI chip stock might be better positioned for long-term gains [4].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reasoning**:\n",
        "#### - The previous response was truncated. I need to complete the output from the previous code block.\n"
      ],
      "metadata": {
        "id": "5vuDtgGRxUJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Answers a company news question by fetching news and stock data,\n",
        "    sending it to Gemini, and parsing the combined response into\n",
        "    news-based, data-based, and combined answers.\n",
        "    \"\"\"\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return {\"error\": \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"}\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    if not tickers:\n",
        "        return {\"error\": \"Could not identify a ticker for the query. Please specify a company name or ticker.\"}\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news articles\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no news articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback for news\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        dprint(f\"News fetch error: {e}\")\n",
        "        all_news = []\n",
        "\n",
        "    news_context = build_context_snippets(all_news)\n",
        "\n",
        "    # Fetch and process stock data (using the first resolved ticker)\n",
        "    stock_data = {}\n",
        "    try:\n",
        "        df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "        stock_data = process_stock_data(df)\n",
        "    except RuntimeError as e:\n",
        "        dprint(f\"Stock data fetch or processing error: {e}\")\n",
        "        stock_data = {}\n",
        "\n",
        "    data_context = json.dumps(stock_data, indent=2) if stock_data else \"No historical stock data available.\"\n",
        "\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst. Answer the user's question based *only* on the provided information (news articles and historical stock data).\\n\"\n",
        "        \"Do NOT use any external knowledge.\\n\"\n",
        "        \"Provide three distinct answers:\\n\"\n",
        "        \"1. News-Based Answer: Based *only* on the provided news articles. Cite articles using [citation number].\\n\"\n",
        "        \"2. Data-Based Answer: Based *only* on the provided historical stock data. Include specific values and date ranges mentioned in the data where relevant.\\n\"\n",
        "        \"3. Combined Answer: Synthesize insights from *both* the news articles and the historical stock data to provide a comprehensive answer to the user's question.\\n\\n\"\n",
        "        \"If the requested information cannot be found in either the news or the data, state that clearly in the relevant section.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Articles:\\n{news_context if news_context else 'No news articles provided.'}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = client.models.generate_content(\n",
        "            model=GEMINI_MODEL,\n",
        "            contents=qa_prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.2,\n",
        "                max_output_tokens=900,\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "            ),\n",
        "        )\n",
        "        raw_response_text = resp.text\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error generating AI response: {e}\"}\n",
        "\n",
        "    # Parse the response\n",
        "    news_answer = \"Not found.\"\n",
        "    data_answer = \"Not found.\"\n",
        "    combined_answer = \"Not found.\"\n",
        "\n",
        "    # Use markers to split the response\n",
        "    news_marker = \"1. News-Based Answer:\"\n",
        "    data_marker = \"2. Data-Based Answer:\"\n",
        "    combined_marker = \"3. Combined Answer:\"\n",
        "\n",
        "    # Find the start of each section\n",
        "    news_start = raw_response_text.find(news_marker)\n",
        "    data_start = raw_response_text.find(data_marker)\n",
        "    combined_start = raw_response_text.find(combined_marker)\n",
        "\n",
        "    # Extract content based on markers\n",
        "    if news_start != -1:\n",
        "        news_content_start = news_start + len(news_marker)\n",
        "        if data_start != -1:\n",
        "            news_answer = raw_response_text[news_content_start:data_start].strip()\n",
        "        elif combined_start != -1:\n",
        "             news_answer = raw_response_text[news_content_start:combined_start].strip()\n",
        "        else:\n",
        "             news_answer = raw_response_text[news_content_start:].strip()\n",
        "\n",
        "\n",
        "    if data_start != -1:\n",
        "        data_content_start = data_start + len(data_marker)\n",
        "        if combined_start != -1:\n",
        "            data_answer = raw_response_text[data_content_start:combined_start].strip()\n",
        "        else:\n",
        "            data_answer = raw_response_text[data_content_start:].strip()\n",
        "\n",
        "\n",
        "    if combined_start != -1:\n",
        "        combined_content_start = combined_start + len(combined_marker)\n",
        "        combined_answer = raw_response_text[combined_content_start:].strip()\n",
        "\n",
        "    return {\n",
        "        \"news_based_answer\": news_answer,\n",
        "        \"data_based_answer\": data_answer,\n",
        "        \"combined_answer\": combined_answer\n",
        "    }\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        response = answer_company_news_question(q)\n",
        "        if \"error\" in response:\n",
        "            print(response[\"error\"])\n",
        "        else:\n",
        "            print(\"--- News-Based Answer ---\")\n",
        "            print(response[\"news_based_answer\"])\n",
        "            print(\"\\n--- Data-Based Answer ---\")\n",
        "            print(response[\"data_based_answer\"])\n",
        "            print(\"\\n--- Combined Answer ---\")\n",
        "            print(response[\"combined_answer\"])\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwQ69vAdxX3y",
        "outputId": "5d123587-f011-4a6d-81dd-7cb5eb39929e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: NVDA\n",
            "\n",
            "Thinking...\n",
            "\n",
            "--- News-Based Answer ---\n",
            "**\n",
            "Nvidia (NVDA) has recently reported second-quarter earnings that exceeded analysts' forecasts, driven by AI data center construction and demand for its new Blackwell chip [2]. The company's performance contributed to a positive sentiment in consumer tech news, with Nvidia \"shining\" during the period of August 25-30 [11]. However, there are concerns about its revenue dependency, as two undisclosed customers accounted for 39% of its Q2 revenue [2]. Despite this, Nvidia is considered a high-quality growth stock that can strengthen long-term investment returns [1] and is predicted to soar over the next five years as it begins to tap into a multitrillion-dollar opportunity [6]. While Nvidia continues to capture headlines, some sources suggest that another critical enabler of the AI infrastructure boom might be better positioned for long-term gains, implying it might not be the top choice for an AI chip stock over the next 10 years [4]. Additionally, Alibaba Group is making moves to reduce its reliance on Nvidia Corporation [7].\n",
            "\n",
            "**\n",
            "\n",
            "--- Data-Based Answer ---\n",
            "**\n",
            "Nvidia's latest close price was $174.18. On that day, the stock opened at $178.11, reached a high of $178.15, and a low of $173.145. The daily change was a decrease of $3.93, representing a -2.2065% change, with a volume of 243,257,873.0 shares. Over the provided data range from 2025-08-18 to 2025-08-29, Nvidia's highest close price was $182.01 on 2025-08-18, and its lowest close price was $174.18 on 2025-08-29. The data includes 10 data points. Information regarding current MA50 and MA200 is not available in the provided data.\n",
            "\n",
            "**\n",
            "\n",
            "--- Combined Answer ---\n",
            "**\n",
            "Nvidia (NVDA) has recently demonstrated strong financial performance, with its Q2 earnings exceeding forecasts, largely due to demand for its Blackwell chip and AI data center construction [2]. This positive news aligns with predictions that Nvidia is a growth stock poised to soar over the next five years, tapping into a multitrillion-dollar opportunity [1, 6]. However, a significant portion (39%) of its Q2 revenue is concentrated with two undisclosed customers, raising concerns about dependency [2]. This dependency, coupled with Alibaba's efforts to reduce reliance on Nvidia [7], suggests potential future challenges despite the current strong performance.\n",
            "\n",
            "From a stock performance perspective, the latest available data shows NVDA closing at $174.18, representing a daily decrease of $3.93 or -2.2065% from its opening price of $178.11. This latest close price on 2025-08-29 is also the lowest close price recorded within the provided data range, which began on 2025-08-18. The highest close price during this period was $182.01 on 2025-08-18. While the news indicates a generally positive outlook and strong earnings, the recent stock data shows a decline to the lowest close price in the provided short-term history, suggesting some recent downward pressure despite the overall bullish sentiment from the articles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reasoning**:\n",
        "#### - Modify the print statements in the `__main__` block to use more descriptive headers and add clear visual separation between the sections, as well as an introductory sentence."
      ],
      "metadata": {
        "id": "UGMsmokixgOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Answers a company news question by fetching news and stock data,\n",
        "    sending it to Gemini, and parsing the combined response into\n",
        "    news-based, data-based, and combined answers.\n",
        "    \"\"\"\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return {\"error\": \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"}\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    if not tickers:\n",
        "        return {\"error\": \"Could not identify a ticker for the query. Please specify a company name or ticker.\"}\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news articles\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no news articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback for news\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        dprint(f\"News fetch error: {e}\")\n",
        "        all_news = []\n",
        "\n",
        "    news_context = build_context_snippets(all_news)\n",
        "\n",
        "    # Fetch and process stock data (using the first resolved ticker)\n",
        "    stock_data = {}\n",
        "    try:\n",
        "        df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "        stock_data = process_stock_data(df)\n",
        "    except RuntimeError as e:\n",
        "        dprint(f\"Stock data fetch or processing error: {e}\")\n",
        "        stock_data = {}\n",
        "\n",
        "    data_context = json.dumps(stock_data, indent=2) if stock_data else \"No historical stock data available.\"\n",
        "\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst. Answer the user's question based *only* on the provided information (news articles and historical stock data).\\n\"\n",
        "        \"Do NOT use any external knowledge.\\n\"\n",
        "        \"Provide three distinct answers:\\n\"\n",
        "        \"1. News-Based Answer: Based *only* on the provided news articles. Cite articles using [citation number].\\n\"\n",
        "        \"2. Data-Based Answer: Based *only* on the provided historical stock data. Include specific values and date ranges mentioned in the data where relevant.\\n\"\n",
        "        \"3. Combined Answer: Synthesize insights from *both* the news articles and the historical stock data to provide a comprehensive answer to the user's question.\\n\\n\"\n",
        "        \"If the requested information cannot be found in either the news or the data, state that clearly in the relevant section.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Articles:\\n{news_context if news_context else 'No news articles provided.'}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = client.models.generate_content(\n",
        "            model=GEMINI_MODEL,\n",
        "            contents=qa_prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.2,\n",
        "                max_output_tokens=900,\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "            ),\n",
        "        )\n",
        "        raw_response_text = resp.text\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error generating AI response: {e}\"}\n",
        "\n",
        "    # Parse the response\n",
        "    news_answer = \"Not found.\"\n",
        "    data_answer = \"Not found.\"\n",
        "    combined_answer = \"Not found.\"\n",
        "\n",
        "    # Use markers to split the response\n",
        "    news_marker = \"1. News-Based Answer:\"\n",
        "    data_marker = \"2. Data-Based Answer:\"\n",
        "    combined_marker = \"3. Combined Answer:\"\n",
        "\n",
        "    # Find the start of each section\n",
        "    news_start = raw_response_text.find(news_marker)\n",
        "    data_start = raw_response_text.find(data_marker)\n",
        "    combined_start = raw_response_text.find(combined_marker)\n",
        "\n",
        "    # Extract content based on markers\n",
        "    if news_start != -1:\n",
        "        news_content_start = news_start + len(news_marker)\n",
        "        if data_start != -1:\n",
        "            news_answer = raw_response_text[news_content_start:data_start].strip()\n",
        "        elif combined_start != -1:\n",
        "             news_answer = raw_response_text[news_content_start:combined_start].strip()\n",
        "        else:\n",
        "             news_answer = raw_response_text[news_content_start:].strip()\n",
        "\n",
        "\n",
        "    if data_start != -1:\n",
        "        data_content_start = data_start + len(data_marker)\n",
        "        if combined_start != -1:\n",
        "            data_answer = raw_response_text[data_content_start:combined_start].strip()\n",
        "        else:\n",
        "            data_answer = raw_response_text[data_content_start:].strip()\n",
        "\n",
        "\n",
        "    if combined_start != -1:\n",
        "        combined_content_start = combined_start + len(combined_marker)\n",
        "        combined_answer = raw_response_text[combined_content_start:].strip()\n",
        "\n",
        "    return {\n",
        "        \"news_based_answer\": news_answer,\n",
        "        \"data_based_answer\": data_answer,\n",
        "        \"combined_answer\": combined_answer\n",
        "    }\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        response = answer_company_news_question(q)\n",
        "        if \"error\" in response:\n",
        "            print(response[\"error\"])\n",
        "        else:\n",
        "            print(\"Here is the analysis based on the available information:\")\n",
        "            print(\"\\n--- News-Based Insights ---\")\n",
        "            print(response.get(\"news_based_answer\", \"N/A\"))\n",
        "            print(\"\\n--- Stock Data Analysis ---\")\n",
        "            print(response.get(\"data_based_answer\", \"N/A\"))\n",
        "            print(\"\\n--- Unified Trend Summary ---\")\n",
        "            print(response.get(\"combined_answer\", \"N/A\"))\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBk9vw6vxkl3",
        "outputId": "5c9ec326-c990-4df6-d470-955f396341b6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: NVDA\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Here is the analysis based on the available information:\n",
            "\n",
            "--- News-Based Insights ---\n",
            "Nvidia (NVDA) is highlighted as a high-quality growth stock that can strengthen long-term investment returns [1]. It is predicted to soar over the next five years, as it is only beginning to tap into a multitrillion-dollar opportunity [6]. Nvidia recently reported second-quarter earnings that exceeded analysts' forecasts, fueled by AI data center construction and demand for its new Blackwell chip [2]. Nvidia's performance was also noted as shining in consumer tech news from August 25-30 [11].\n",
            "\n",
            "However, there are some concerns regarding Nvidia's revenue dependency, as two undisclosed customers drive 39% of its Q2 revenue [2]. Additionally, while Nvidia captures headlines, one article suggests that another critical enabler of the AI infrastructure boom may be better positioned for long-term gains, implying it's not Nvidia [4]. Alibaba Group is also making moves to reduce its reliance on Nvidia Corporation [7].\n",
            "\n",
            "###\n",
            "\n",
            "--- Stock Data Analysis ---\n",
            "Nvidia's latest closing price was $174.18. On that day, it opened at $178.11, reached a high of $178.15, and a low of $173.145. The daily change was a decrease of $3.93, representing a -2.2065% change. The latest volume was 243,257,873.\n",
            "\n",
            "Over the provided data range from August 18, 2025, to August 29, 2025, Nvidia's highest closing price was $182.01 on August 18, 2025. Its lowest closing price was $174.18 on August 29, 2025. There are 10 data points available in this period. The current 50-day and 200-day moving averages are not available in the provided data.\n",
            "\n",
            "###\n",
            "\n",
            "--- Unified Trend Summary ---\n",
            "Nvidia (NVDA) has recently demonstrated strong performance, with its second-quarter earnings exceeding analyst forecasts, driven by the demand for its Blackwell chip and AI data center construction [2]. This positive momentum is reflected in its mention as a growth stock to buy and hold [1], with predictions of it soaring over the next five years due to a multitrillion-dollar opportunity [6]. Nvidia's strong showing was also noted in consumer tech news for the last week of August [11].\n",
            "\n",
            "However, the historical stock data shows that despite these positive news items, NVDA experienced a recent decline. Its latest closing price of $174.18 on August 29, 2025, was also its lowest closing price within the provided data range (August 18-29, 2025). This represents a daily decrease of $3.93 or -2.2065% from its opening price, and a drop from its highest close of $182.01 on August 18, 2025.\n",
            "\n",
            "While the news suggests a bullish long-term outlook for Nvidia, concerns exist regarding its heavy reliance on two undisclosed customers for 39% of its Q2 revenue [2]. Furthermore, some articles suggest that other AI chip stocks might be better positioned for long-term gains [4], and competitors like Alibaba are actively working to reduce their dependence on Nvidia [7]. The recent dip in the stock price, as seen in the data, could potentially be a short-term fluctuation or a reaction to these underlying concerns, despite the overall positive sentiment from some analysts in the news.\n"
          ]
        }
      ]
    }
  ]
}
